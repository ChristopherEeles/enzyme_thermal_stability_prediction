{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novozyme Enzyme Stability Prediction\n",
    "\n",
    "This notebook contains model training and evaluation to predict the thermal stability (as measured via melting point) of enzymes based on their amino acid sequence.\n",
    "\n",
    "Competition details are available [here](https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepared for SCS3546 - Deep Learning\n",
    "\n",
    "<pre> Christopher Eeles </pre>\n",
    "\n",
    "<pre> X361483 </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the dependencies for this notebook are available in a Conda\n",
    "environment file on GitHub under `ChristopherEeles/enzyme_thermal_stability_prediction/env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Download\n",
    "\n",
    "Retrieve the dataset from Kaggle via the Kaggle API utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from shlib import Cmd\n",
    "import zipfile as zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path constants\n",
    "DATA_DIR = Path(\"rawdata\")\n",
    "METADATA_DIR = Path(\"metadata\")\n",
    "LOG_DIR = Path(\"logs\")\n",
    "RESULT_DIR = Path(\"results\")\n",
    "\n",
    "# Kaggle constants\n",
    "COMPETITION_NAME = \"novozymes-enzyme-stability-prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize project directories\n",
    "for d in (DATA_DIR, METADATA_DIR, LOG_DIR, RESULT_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading novozymes-enzyme-stability-prediction.zip to rawdata\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.06M/7.06M [00:00<00:00, 49.4MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('rawdata/novozymes-enzyme-stability-prediction.zip')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download competition data\n",
    "download_competition_files = Cmd([\"kaggle\", \"competitions\", \"download\", \"-c\", \n",
    "    COMPETITION_NAME, \"-p\", DATA_DIR])\n",
    "download_competition_files.run()\n",
    "dataset_file = sorted(DATA_DIR.glob(f\"{COMPETITION_NAME}.*\"))\n",
    "dataset_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zip.ZipFile(dataset_file[0].resolve()) as z:\n",
    "    z.extractall(path=DATA_DIR)\n",
    "    dataset_file[0].unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('rawdata/sample_submission.csv'),\n",
       " PosixPath('rawdata/test.csv'),\n",
       " PosixPath('rawdata/train.csv'),\n",
       " PosixPath('rawdata/train_updates_20220929.csv'),\n",
       " PosixPath('rawdata/wildtype_structure_prediction_af2.pdb')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_files = sorted(DATA_DIR.glob(\"*\"))\n",
    "dataset_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Before we begin modelling we will have a look at the files available for the Novozyme competition to see what kind of features are available to help with our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from biopandas.pdb import PandasPdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = dataset_files[2]\n",
    "TEST_PATH = dataset_files[1]\n",
    "SAMPLE_SUBMISSION = dataset_files[0]\n",
    "TRAIN_UPDATE_PATH = dataset_files[3]\n",
    "TRAIN_PDB_PATH = dataset_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load available csv files\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "sample_submission_df = pd.read_csv(SAMPLE_SUBMISSION)\n",
    "train_update_df = pd.read_csv(TRAIN_UPDATE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Biopythons Biopandas to load the PDB protein structure file\n",
    "pdb_df = PandasPdb().read_pdb(str(TRAIN_PDB_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ATOM', 'HETATM', 'ANISOU', 'OTHERS'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get it PDB file into a Python native format\n",
    "protein_struct_df_dict = pdb_df.df\n",
    "protein_struct_df_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31390 entries, 0 to 31389\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   seq_id            31390 non-null  int64  \n",
      " 1   protein_sequence  31390 non-null  object \n",
      " 2   pH                31104 non-null  float64\n",
      " 3   data_source       28043 non-null  object \n",
      " 4   tm                31390 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>data_source</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>40.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>47.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_id                                   protein_sequence   pH  \\\n",
       "0       0  AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...  7.0   \n",
       "1       1  AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...  7.0   \n",
       "2       2  AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...  7.0   \n",
       "3       3  AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0   \n",
       "4       4  AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...  7.0   \n",
       "\n",
       "                         data_source    tm  \n",
       "0  doi.org/10.1038/s41592-020-0801-4  75.7  \n",
       "1  doi.org/10.1038/s41592-020-0801-4  50.5  \n",
       "2  doi.org/10.1038/s41592-020-0801-4  40.5  \n",
       "3  doi.org/10.1038/s41592-020-0801-4  47.2  \n",
       "4  doi.org/10.1038/s41592-020-0801-4  49.5  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE: tm column is melting point in Celsius (C)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2434 entries, 0 to 2433\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   seq_id            2434 non-null   int64  \n",
      " 1   protein_sequence  25 non-null     object \n",
      " 2   pH                25 non-null     float64\n",
      " 3   data_source       0 non-null      float64\n",
      " 4   tm                25 non-null     float64\n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 95.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train_update_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>data_source</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_id protein_sequence  pH  data_source  tm\n",
       "0      69              NaN NaN          NaN NaN\n",
       "1      70              NaN NaN          NaN NaN\n",
       "2      71              NaN NaN          NaN NaN\n",
       "3      72              NaN NaN          NaN NaN\n",
       "4      73              NaN NaN          NaN NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There were some data quality issues, need to drop NaN rows and update some pH and tm values\n",
    "# See: https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction/discussion/356251\n",
    "train_update_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   69,    70,    71, ..., 30740, 30741, 30742])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract rows which need updating in train data\n",
    "bad_seq_ids = train_update_df.seq_id.values\n",
    "bad_seq_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop those rows from the training data and append the updated rows\n",
    "train_df_fix = train_df.loc[~train_df.seq_id.isin(bad_seq_ids), :]\n",
    "train_df_fix = (pd.concat([train_df_fix, train_update_df])\n",
    "    .sort_values(by=\"seq_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>protein_sequence</th>\n",
       "      <th>pH</th>\n",
       "      <th>data_source</th>\n",
       "      <th>tm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>75.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>40.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>47.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>doi.org/10.1038/s41592-020-0801-4</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   seq_id                                   protein_sequence   pH  \\\n",
       "0       0  AAAAKAAALALLGEAPEVVDIWLPAGWRQPFRVFRLERKGDGVLVG...  7.0   \n",
       "1       1  AAADGEPLHNEEERAGAGQVGRSLPQESEEQRTGSRPRRRRDLGSR...  7.0   \n",
       "2       2  AAAFSTPRATSYRILSSAGSGSTRADAPQVRRLHTTRDLLAKDYYA...  7.0   \n",
       "3       3  AAASGLRTAIPAQPLRHLLQPAPRPCLRPFGLLSVRAGSARRSGLL...  7.0   \n",
       "4       4  AAATKSGPRRQSQGASVRTFTPFYFLVEPVDTLSVRGSSVILNCSA...  7.0   \n",
       "\n",
       "                         data_source    tm  \n",
       "0  doi.org/10.1038/s41592-020-0801-4  75.7  \n",
       "1  doi.org/10.1038/s41592-020-0801-4  50.5  \n",
       "2  doi.org/10.1038/s41592-020-0801-4  40.5  \n",
       "3  doi.org/10.1038/s41592-020-0801-4  47.2  \n",
       "4  doi.org/10.1038/s41592-020-0801-4  49.5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_fix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check the non NaN columns got updated correctly\n",
    "assert all(train_df_fix.iloc[bad_seq_ids].pH.dropna() == train_update_df.pH.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with NaN in the tm column, since that is our target in modelling\n",
    "train_df2 = train_df_fix.loc[~train_df_fix.tm.isna(), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28981 entries, 0 to 31389\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   seq_id            28981 non-null  int64  \n",
      " 1   protein_sequence  28981 non-null  object \n",
      " 2   pH                28695 non-null  float64\n",
      " 3   data_source       28001 non-null  object \n",
      " 4   tm                28981 non-null  float64\n",
      "dtypes: float64(2), int64(1), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to memory constraints need to only work with shorter proteins!\n",
    "train_df2 = train_df2.loc[train_df2.protein_sequence.apply(lambda x: len(x)) < 1000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27029, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.protein_sequence.apply(lambda x: len(x)).max()  # Check longest sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write train/test data as a FASTA sequence file, makes it easier to make DataLoader for PyTorch\n",
    "TRAIN_FASTA = RESULT_DIR / \"train.fasta\"\n",
    "with TRAIN_FASTA.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(train_df2.shape[0]):\n",
    "        row = train_df2.iloc[i, :]\n",
    "        f.write(f\">{row.seq_id}|{row.pH}|{row.tm}\\n\")\n",
    "        f.write(f\"{row.protein_sequence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2413 entries, 0 to 2412\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   seq_id            2413 non-null   int64 \n",
      " 1   protein_sequence  2413 non-null   object\n",
      " 2   pH                2413 non-null   int64 \n",
      " 3   data_source       2413 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 75.5+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated proteins in test set!\n",
    "assert len(test_df.protein_sequence.unique()) == test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FASTA = RESULT_DIR / \"test.fasta\"\n",
    "with TEST_FASTA.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for i in range(test_df.shape[0]):\n",
    "        row = test_df.iloc[i, :]\n",
    "        f.write(f\">{row.seq_id}|{row.pH}|\\n\")\n",
    "        f.write(f\"{row.protein_sequence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the relatively small training set of proteins for this competition, it\n",
    "is likely optimal to leverage an existing model via transfer learning to ensure\n",
    "we can extract sufficient biochemical context from the protein sequences to\n",
    "usefully rank the proteins by thermal stability.\n",
    "\n",
    "Based on a brief review of the literature, I identifier the \n",
    "Evolutionary Scale Modelling (ESM) tranformer architecture from Facebook Research\n",
    "as a potential candidate for transfer learning to the thermal stability task.\n",
    "Pre-trained versions of their model are available via their PyPI package\n",
    "`fair-esm`. The model is implemented in PyTorch and given my limited exposure\n",
    "to this deep learning framework I spent a long time in trail and error\n",
    "before I could succesfully export the latent space embeddings of the protein\n",
    "sequences I would use for downstream modelling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn, optim, utils, Tensor\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Facebook Research Evolutionary Scale Modelling (ESM) package\n",
    "from esm import FastaBatchedDataset\n",
    "import esm\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "import inspect\n",
    "import time\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, alpha = esm.pretrained.esm2_t30_150M_UR50D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2Regressor(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiLayerPerceptron model for regression appended to an ESM2 model for\n",
    "    feature extraction for protein sequence data.\n",
    "    \"\"\"\n",
    "    def __init__(self, esm2 = esm.pretrained.esm2_t30_150M_UR50D, \n",
    "        freeze_esm = True, layers = [64, 32, 16], return_contacts=False, \n",
    "        repr_layers=30\n",
    "    ):\n",
    "        super(ESM2Regressor, self).__init__()\n",
    "        # Initialize our pretrained ESM2 model and alphabet\n",
    "        esm_model, esm_alphabet = esm2()\n",
    "        # Freeze model weights if not doing fine tuning\n",
    "        if freeze_esm:\n",
    "            for p in esm_model.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.freeze_esm = freeze_esm\n",
    "        self.esm_alphabet = esm_alphabet\n",
    "        self.feature_extractor = esm_model\n",
    "        self.return_contacts = return_contacts\n",
    "        self.repr_layers = repr_layers\n",
    "        # Get the output shape from our ESM2 feature extractor\n",
    "        self.input_dim = (list(self.feature_extractor.children())[-1]\n",
    "            .dense.out_features)\n",
    "        # Add our regression MLP, parameterizing the layers\n",
    "        self.fc = nn.Sequential()\n",
    "        previous_l = self.input_dim\n",
    "        for l in layers:\n",
    "            #self.fc.append(nn.BatchNorm1d(previous_l))\n",
    "            self.fc.append(nn.Linear(previous_l, l))\n",
    "            self.fc.append(nn.ReLU())\n",
    "            previous_l = l\n",
    "        self.fc.append(\n",
    "            nn.Linear(previous_l, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        features = self.feature_extractor(input, repr_layers=[self.repr_layers],\n",
    "            return_contacts=self.return_contacts\n",
    "        )\n",
    "        # Sum along token dimension to get representation per sequence\n",
    "        x = features[\"representations\"][self.repr_layers].sum(1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "REPR_LAYERS = 30\n",
    "BATCH_SIZE = 32\n",
    "RETURN_CONTACTS = False  # To save on memory usage, even though contacts may be useful for stability prediction\n",
    "VAL_SPLIT = 0.2\n",
    "TRAIN_SPLIT = 1 - VAL_SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2Regressor(\n",
       "  (feature_extractor): ESM2(\n",
       "    (embed_tokens): Embedding(33, 640, padding_idx=1)\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (23): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (24): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (25): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (26): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (27): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (28): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (29): TransformerLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "          (rot_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "        (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "        (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (contact_head): ContactPredictionHead(\n",
       "      (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "    (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    (lm_head): RobertaLMHead(\n",
       "      (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "      (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ESM2Regressor()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get batch converter to preprocess our data from the dataloader\n",
    "batch_converter = model.esm_alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in our entire training set\n",
    "dataset = FastaBatchedDataset.from_file(TRAIN_FASTA)\n",
    "# Determine size for our train-test split\n",
    "data_size = len(dataset)\n",
    "TRAIN_SIZE = int(np.floor(data_size * TRAIN_SPLIT))\n",
    "VAL_SIZE = data_size - TRAIN_SIZE\n",
    "# Set seeds\n",
    "torch.manual_seed(1990 * 42)\n",
    "np.random.seed(1990 * 42)\n",
    "# Shuffle data indices for subsetting\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)  # shuffles by reference\n",
    "# Configure traning and validation samplers\n",
    "train_idx, val_idx = indices[:TRAIN_SIZE], indices[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training and validation samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, collate_fn=batch_converter, sampler=val_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, collate_fn=batch_converter, sampler=val_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"val\": val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = FastaBatchedDataset.from_file(TEST_FASTA)\n",
    "test_batches = test_dataset.get_batch_indices(BATCH_SIZE, extra_toks_per_seq=1)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, collate_fn=batch_converter, batch_sampler=test_batches, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from PyTorch docs: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "# TensorBoard code adapted from docs: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, device, \n",
    "    num_epochs=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to train our ESMRegressor model with validation.\n",
    "    \"\"\"\n",
    "    ## TODO:: Add tensorboard logging!\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 0.0\n",
    "\n",
    "    datasizes = {k: len(v) for k, v in dataloaders.items()}\n",
    "\n",
    "    model = model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # training and validation phase for each epoch\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "                # freeze the ESM2 model parameters, very expensive to train\n",
    "                if model.freeze_esm:\n",
    "                    for p in model.feature_extractor.parameters():\n",
    "                        p.require_grad = False\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for labels, _, inputs in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                # last value in string is melting point for each protein\n",
    "                labels = Tensor([float(s.split(\"|\")[-1]) for s in labels])\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward pass\n",
    "                # track history only for training\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels.reshape([len(labels), 1]))\n",
    "\n",
    "                    # backward pass + optimization for training only\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # compute running statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        if phase == \"train\":\n",
    "            # Adjust learning rate schedule per epoch\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / datasizes[phase]\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        if phase == \"val\" and epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    print()  # Add newline before next epoch\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val loss: {best_loss:4f}')\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Select an optimizer, only optimzie fully connected layer paramters to speed up training\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.01, weight_decay=0.001)\n",
    "\n",
    "# Use learning rate scheduler, reduce lr by order of magniture per 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "----------\n",
      "val Loss: 2775.4218\n",
      "Epoch 2/100\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train_model(model, criterion, optimizer, scheduler, dataloaders, device, \n\u001b[1;32m      2\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [42], line 56\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, dataloaders, device, num_epochs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     55\u001b[0m         \u001b[39m# compute running statistics\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m         running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# Adjust learning rate schedule per epoch\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, scheduler, dataloaders, device, \n",
    "    num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, strs, toks = next(iter(train_dataloader))\n",
    "print(toks.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('lightning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca5f1d89428e714e633aee0d2b0101afffaa1b5637385fe4acc1c55c7ad9a1a0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
